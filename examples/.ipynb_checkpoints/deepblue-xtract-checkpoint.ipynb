{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Xtract to extract MaterialsIO metadata from MDF files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Xtract-MDF demo illustrates how to crawl, extract metadata from, and store metadata for any Globus HTTPS-accessible repository. \n",
    "\n",
    "Moreover, users can execute metadata extraction workflows on any machine running a funcX endpoint, whether it's ANL's Cooley, a laptop, or the cloud (for this demo, we use an EC2 instance). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fair_research_login import NativeClient\n",
    "import requests\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# Globus endpoint and directory path to where the data live (default: MDF data repo on petrel#researchdatalanalytics).\n",
    "# source_ep_id = \"e38ee745-6d04-11e5-ba46-22000b92c6ec\"  # where the data are\n",
    "#source_ep_path = \"/thurston_selfassembled_peptide_spectra_v1.1/DFT/MoleculeConfigs/di_30_-10.xyz/\"\n",
    "source_ep_path = \"/UMich\"\n",
    "\n",
    "source_ep_id = \"4f99675c-ac1f-11ea-bee8-0e716405a293\"\n",
    "dest_ep_id = \"1adf6602-3e50-11ea-b965-0e16720bb42f\"  # where they will be extracted\n",
    "\n",
    "# Globus endpoint and file path at which we want to store metadata documents\n",
    "mdata_ep_id = \"5113667a-10b4-11ea-8a67-0e35e66293c2\"\n",
    "mdata_path = \"/projects/DLHub/mdf_metadata\"  # TODO: Add exception if you put slash at the end of the mdata_path. \n",
    "\n",
    "# FuncX endpoint at which we want the metadata extraction to occur. Does NOT have to be same endpoint as the data.\n",
    "#. funcx_ep_id = \"6045fcfb-c3ef-48db-9b32-5b50fda15144\"  # Path to funcX running on JetStream. \n",
    "funcx_ep_id = \"82ceed9f-dce1-4dd1-9c45-6768cf202be8\"  # River k8s cluster. \n",
    "\n",
    "# base_url = \"https://data.materialsdatafacility.org\"\n",
    "\n",
    "# URLs where the Xtract service (AWS Elastic Beanstalk) AND the metadata poller service (AWS EC2) run\n",
    "# eb_url = \"http://xtractv1-env-2.p6rys5qcuj.us-east-1.elasticbeanstalk.com\"\n",
    "eb_url = \"http://127.0.0.1:5000\"\n",
    "# eb_url = \"http://xtract-crawler-4.eba-ghixpmdf.us-east-1.elasticbeanstalk.com\"\n",
    "# poller_url = \"http://ec2-54-173-234-195.compute-1.amazonaws.com\"\n",
    "\n",
    "# Grouping strategy we want to use for grouping. This will, by default, use all .group() functions from matio parsers.\n",
    "grouper = \"matio\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Login \n",
    "\n",
    "Here we request tokens from Globus Auth coming from three separate scopes. When fresh tokens are needed, tthe NativeClient will provide a link at which the user can authenticate with their Globus ID, providing a box at which to paste the Authentication Code. The scopes are as follows: \n",
    "\n",
    "* **petrel_https_server**: needed to access the MDF data on Petrel. Will need to change if processing data off-Petrel. \n",
    "* **transfer_token**: needed to crawl the Globus endpoint and transfer metadata to its final location. \n",
    "* **funcx_token**: needed to orchestrate the metadata exraction at the given funcX endpoint.\n",
    "\n",
    "Additionally we package the tokens as *headers* that we can easily ship with later requests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headers: {'Authorization': 'Bearer AgYvVXxwKxx2n1N4aEpG1vJ6Vy6djl9Yyqeqe8V9leV282Y4K7tWCVvpMEwY9X2Bjl0Gpn41DKvb2jSVo3kdkh1kaz', 'Transfer': 'AgWaXN28yxOMv4MoX7EbP7nGBw49DGp3bbdvMx3xekwlwEnovWSgCDPJVOeYr25KlDMYeOY7GvXnBVU4E8wqwIKWxd', 'FuncX': 'AgYozx6KG04NDoYGYx4VDPx4YvK8k8Qx6pmgvModz0l33v2nneSWCo2XkWO26yeW3QPoMPJoYWxq1PiVo3kdkhE2Ja', 'Petrel': 'AgYvVXxwKxx2n1N4aEpG1vJ6Vy6djl9Yyqeqe8V9leV282Y4K7tWCVvpMEwY9X2Bjl0Gpn41DKvb2jSVo3kdkh1kaz'}\n"
     ]
    }
   ],
   "source": [
    "client = NativeClient(client_id='7414f0b4-7d05-4bb6-bb00-076fa3f17cf5')\n",
    "tokens = client.login(\n",
    "    requested_scopes=['https://auth.globus.org/scopes/56ceac29-e98a-440a-a594-b41e7a084b62/all', \n",
    "                      'urn:globus:auth:scope:transfer.api.globus.org:all',\n",
    "                     \"https://auth.globus.org/scopes/facd7ccc-c5f4-42aa-916b-a0e270e2c2a9/all\", \n",
    "                     'email', 'openid'],\n",
    "    no_local_server=True,\n",
    "    no_browser=True)\n",
    "\n",
    "auth_token = tokens[\"petrel_https_server\"]['access_token']\n",
    "transfer_token = tokens['transfer.api.globus.org']['access_token']\n",
    "funcx_token = tokens['funcx_service']['access_token']\n",
    "\n",
    "headers = {'Authorization': f\"Bearer {auth_token}\", 'Transfer': transfer_token, 'FuncX': funcx_token, 'Petrel': auth_token}\n",
    "print(f\"Headers: {headers}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Crawl\n",
    "Crawling, behind the scenes, will scan a Globus directory breadth-first (using globus_ls), first extracting physical metadata such as path, size, and extension. Next, since the *grouper* we selected is 'matio', the crawler will execute matio's `get_groups_by_postfix()` function on all file names in a directory in order to return groups for each of matio's parsers (besides *generic* and *noop*). \n",
    "\n",
    "The crawl will run as a non-blocking thread, and return a crawl_id that will be used extensively to track progress of our metadata extraction workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawl URL is : http://127.0.0.1:5000/crawl\n",
      "b'{\"crawl_id\":\"0138ec96-5512-4c80-95fd-96dccd79131d\"}\\n'\n",
      "Crawl ID: 0138ec96-5512-4c80-95fd-96dccd79131d\n"
     ]
    }
   ],
   "source": [
    "# TODO: Adjust this to the Google Drive model!!!\n",
    "\n",
    "crawl_url = f'{eb_url}/crawl'\n",
    "print(f\"Crawl URL is : {crawl_url}\")\n",
    "crawl_req = requests.post(f'{eb_url}/crawl', json={'repo_type': \"GLOBUS\", 'eid': source_ep_id, 'dir_path': source_ep_path, 'Transfer': transfer_token, 'Authorization': funcx_token,'grouper': grouper})\n",
    "print(crawl_req.content)\n",
    "crawl_id = json.loads(crawl_req.content)['crawl_id']\n",
    "print(f\"Crawl ID: {crawl_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get crawl status, seeing how many groups have been identified in the crawl. \n",
    "\n",
    "Note that measuring the total files yet to crawl is impossible, as the BFS may not have discovered all files yet, and Globus does not yet have a file counting feature for all directories and subdirectories. I.e., we know when we're done, but we don't know until we get there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Crawl Status: {'bytes_processed': 33760192, 'crawl_id': '1918f96c-bd4b-4815-876c-d51a00dd40e4', 'crawl_status': 'crawling', 'elapsed_time': 1.919757, 'files_processed': 8, 'groups_crawled': 80}\n"
     ]
    }
   ],
   "source": [
    "# crawl_id\n",
    "crawl_status = requests.get(f'{eb_url}/get_crawl_status', json={'crawl_id': crawl_id})\n",
    "print(crawl_status)\n",
    "crawl_content = json.loads(crawl_status.content)\n",
    "print(f\"Crawl Status: {crawl_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Xtract\n",
    "\n",
    "Next we launch a non-blocking metadata extraction workflow that will automatically find all groups generated from our crawl_id, ship parsers to our endpoint as funcX, transfer the file (if necessary), and extract/send back metadata to the central Xtract service. This will just run constantly until the crawl is done and there are crawled groups left to extract. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtract response (should be 200): <Response [200]>\n"
     ]
    }
   ],
   "source": [
    "xtract = requests.post(f'{eb_url}/extract', json={'crawl_id': crawl_id,\n",
    "                                                  'repo_type': \"HTTPS\",\n",
    "                                                  'headers': json.dumps(headers),\n",
    "                                                  'funcx_eid': funcx_ep_id, \n",
    "                                                  'source_eid': source_ep_id,\n",
    "                                                  'dest_eid': dest_ep_id,\n",
    "                                                  'mdata_store_path': mdata_path})\n",
    "print(f\"Xtract response (should be 200): {xtract}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtract Status: {'FAILED': 0, 'FINISHED': 0, 'IDLE': 0, 'PENDING': 0, 'crawl_id': 'f6512c39-2690-4ac6-b336-4923d3c390ff'}\n"
     ]
    }
   ],
   "source": [
    "xtract_status = requests.get(f'{eb_url}/get_extract_status', json={'crawl_id': crawl_id})\n",
    "xtract_content = json.loads(xtract_status.content)\n",
    "print(f\"Xtract Status: {xtract_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Access / Flush\n",
    "\n",
    "We might want to flush all new metadata blobs to a separate Globus endpoint. Here we initialize a results poller that creates a file of each metadata attribute to a folder at this path: `<mdata_path>/<crawl_id>/<group_id>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flush Status: <Response [200]>\n"
     ]
    }
   ],
   "source": [
    "poller = requests.post(f'{poller_url}/', json={'crawl_id': crawl_id, 'mdata_ep_id': mdata_ep_id, 'Transfer': transfer_token})\n",
    "print(f'Flush Status: {poller}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (funcx-jan)",
   "language": "python",
   "name": "funcx-jan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
