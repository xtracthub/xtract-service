{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling and Extracting Files from Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Crawling\n",
    "...is how we create our initial index of files. Downstream, this lets us strategize as to how many and what types of files are processed at different locations. Here we use the **Google OAuth2 InstalledAppFlow** to get a user's Auth credentials, and then use the **Google Drive API** to generate a list of all files and their attributes from the Google Drive API. \n",
    "\n",
    "You may need to install the Google Drive requirements before running this notebook. In the terminal this can be done with: \n",
    "\n",
    "```pip install -r google_drive_nb_requirements.txt```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# Standard Py Imports\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import os.path\n",
    "import requests\n",
    "\n",
    "# Google Imports\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, set the following elements as environment variables on your machine\n",
    "# ** DISCLAIMER **: if you push these to GitHub, I will be sad.\n",
    "# ** FOR MACOSX: After you source, .bash_profile, might need full system restart to discover env vars. \n",
    "project_id = os.environ[\"goog_project_id\"]\n",
    "client_id = os.environ[\"goog_client_id\"]\n",
    "client_secret = os.environ[\"goog_client_secret\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r_url = \"http://xtract-crawler-4.eba-ghixpmdf.us-east-1.elasticbeanstalk.com/crawl\"\n",
    "r_url = \"http://127.0.0.1:5000/crawl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'web': {'auth_uri': 'https://accounts.google.com/o/oauth2/auth', 'token_uri': 'https://oauth2.googleapis.com/token', 'auth_provider_x509_cert_url': 'https://www.googleapis.com/oauth2/v1/certs', 'redirect_uris': ['http://localhost', 'http://localhost/potato', 'http://localhost:8080/Callback'], 'javascript_origins': ['http://localhost:8080']}}\n"
     ]
    }
   ],
   "source": [
    "# Add the secret stuff to our credentials document...\n",
    "with open(\"config.json\", \"r\") as f:\n",
    "    creds = json.load(f)\n",
    "print(creds)\n",
    "creds['web']['client_id'] = client_id\n",
    "creds['web']['client_secret'] = client_secret\n",
    "creds['web']['project_id'] = project_id\n",
    "\n",
    "# And write it.\n",
    "with open(\"credentials.json\", \"w\") as f: \n",
    "    json.dump(creds, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to define and run the login flow. Here we give our credentials to the Google OAuth2 API server, which will give back an immutable Credentials() object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=364500245041-r1eebsermd1qp1qo68a3qp09hhpc5dfi.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A53556%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.metadata.readonly+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.readonly&state=aFuFr7pHYbgJHrPAX4rqYBBhXgqvZ6&access_type=offline\n"
     ]
    }
   ],
   "source": [
    "SCOPES = ['https://www.googleapis.com/auth/drive.metadata.readonly', 'https://www.googleapis.com/auth/drive.readonly']\n",
    "\n",
    "# Stolen from Google Quickstart docs\n",
    "# https://developers.google.com/drive/api/v3/quickstart/python\n",
    "def do_login_flow():\n",
    "    creds = None\n",
    "    # The file token.pickle stores the user's access and refresh tokens, and is\n",
    "    # created automatically when the authorization flow completes for the first\n",
    "    # time.\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'credentials.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "\n",
    "    return creds, None  # Returning None because Tyler can't figure out how he wants to structure this yet. \n",
    "\n",
    "# THIS should force-open a Google Auth window in your local browser. If not, you can manually copy-paste it. \n",
    "auth_creds = do_login_flow()\n",
    "\n",
    "# Now delete the file so you don't accidentally `git add` it. \n",
    "os.remove(\"credentials.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-1e03dcb8d089>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                   data=pickle.dumps({'auth_creds': auth_creds, 'repo_type': 'GDRIVE'}) )\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcrawl_mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Crawl Started! Here's your trackable crawl_id:\\n {crawl_mdata}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcrawl_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrawl_mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'crawl_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/plz/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/plz/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/plz/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "r = requests.post(url=r_url,\n",
    "                  data=pickle.dumps({'auth_creds': auth_creds, 'repo_type': 'GDRIVE'}) )\n",
    "\n",
    "crawl_mdata = json.loads(r.content)\n",
    "print(f\"Crawl Started! Here's your trackable crawl_id:\\n {crawl_mdata}\")\n",
    "crawl_id = crawl_mdata['crawl_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Extraction\n",
    "...is how we create our initial index of files. Downstream, this lets us strategize as to how many and what types of files are processed at different locations. Here we use the **Google OAuth2 InstalledAppFlow** to get a user's Auth credentials, and then use the **Google Drive API** to generate a list of all files and their attributes from the Google Drive API. \n",
    "\n",
    "You may need to install the Google Drive requirements before running this notebook. In the terminal this can be done with: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bytes_crawled': None, 'crawl_id': '950d4a81-9dec-45a6-91f1-c931386d7426', 'crawl_start_t': 1595102195.8121102, 'crawl_status': 'COMMITTING', 'files_crawled': 3137, 'gdrive_mdata': {'doc_types': {'is_gdoc': 1123, 'is_user_upload': 2014}, 'first_ext_tallies': {'compressed': 6, 'hierarch': 1, 'images': 377, 'other': 298, 'presentation': 143, 'tabular': 200, 'text': 2112}}, 'groups_crawled': 3137, 'n_commit_threads': 5, 'repo_type': 'GDrive'}\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(url='http://127.0.0.1:5000/get_crawl_status', json={'crawl_id': crawl_id})\n",
    "print(json.loads(r.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please paste the following URL in a browser:\n",
      "https://auth.globus.org/v2/oauth2/authorize?client_id=7414f0b4-7d05-4bb6-bb00-076fa3f17cf5&redirect_uri=https%3A%2F%2Fauth.globus.org%2Fv2%2Fweb%2Fauth-code&scope=https%3A%2F%2Fauth.globus.org%2Fscopes%2F56ceac29-e98a-440a-a594-b41e7a084b62%2Fall+urn%3Aglobus%3Aauth%3Ascope%3Atransfer.api.globus.org%3Aall+https%3A%2F%2Fauth.globus.org%2Fscopes%2Ffacd7ccc-c5f4-42aa-916b-a0e270e2c2a9%2Fall+email+openid&state=_default&response_type=code&code_challenge=GrxnqfB-VAmoX-_Fgr5GCw65I3pCiqQgS2DQNtWEsVk&code_challenge_method=S256&access_type=online&prefill_named_grant=My+App+Login\n",
      "Please Paste your Auth Code Below: \n",
      "tzBGJMYzEwknUZLU0O7AsLFEPOKCoc\n",
      "Headers: {'Authorization': 'Bearer Ag1gdPeM4kYnEP3dXpPxN5DY2jM8pxN2w1976Qjnol3njXy435T2Cb3kM87B7eaMqY3lnXzp8plgPqU42gW8ysX091', 'Transfer': 'Agorzvdv349JborEp76o4aNyx1DQM1PkgO5868VrBn8l0kMy0JIgC3O7dObMElM7Gal0xpnDX2PJl8cK18grdH3QaE', 'FuncX': 'Ag6vY8xmMO8B534My1qrjYY80jGVJXmvGvjWExaxq33KXw5j0NF2C3zzm6K74kQmEdX8Byp4r6bBkWs1qQ8GKUVB8n', 'Petrel': 'Ag1gdPeM4kYnEP3dXpPxN5DY2jM8pxN2w1976Qjnol3njXy435T2Cb3kM87B7eaMqY3lnXzp8plgPqU42gW8ysX091'}\n"
     ]
    }
   ],
   "source": [
    "# Do a Globus Login (Will won't need this, because of REFRESH token)\n",
    "from fair_research_login import NativeClient\n",
    "\n",
    "client = NativeClient(client_id='7414f0b4-7d05-4bb6-bb00-076fa3f17cf5')\n",
    "tokens = client.login(\n",
    "    requested_scopes=['https://auth.globus.org/scopes/56ceac29-e98a-440a-a594-b41e7a084b62/all', \n",
    "                      'urn:globus:auth:scope:transfer.api.globus.org:all',\n",
    "                     \"https://auth.globus.org/scopes/facd7ccc-c5f4-42aa-916b-a0e270e2c2a9/all\", \n",
    "                     'email', 'openid'],\n",
    "    no_local_server=True,\n",
    "    no_browser=True)\n",
    "\n",
    "auth_token = tokens[\"petrel_https_server\"]['access_token']\n",
    "transfer_token = tokens['transfer.api.globus.org']['access_token']\n",
    "funcx_token = tokens['funcx_service']['access_token']\n",
    "\n",
    "headers = {'Authorization': f\"Bearer {auth_token}\", 'Transfer': transfer_token, 'FuncX': funcx_token, 'Petrel': auth_token}\n",
    "print(f\"Headers: {headers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bytes_crawled': None, 'crawl_end_t': 1595102293.971693, 'crawl_id': '950d4a81-9dec-45a6-91f1-c931386d7426', 'crawl_start_t': 1595102195.8121102, 'crawl_status': 'COMPLETED', 'files_crawled': 3137, 'gdrive_mdata': {'doc_types': {'is_gdoc': 1123, 'is_user_upload': 2014}, 'first_ext_tallies': {'compressed': 6, 'hierarch': 1, 'images': 377, 'other': 298, 'presentation': 143, 'tabular': 200, 'text': 2112}}, 'groups_crawled': 3137, 'n_commit_threads': 5, 'repo_type': 'GDrive', 'total_crawl_time': 98.15958285331726}\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(url='http://127.0.0.1:5000/get_crawl_status', json={'crawl_id': crawl_id})\n",
    "print(json.loads(r.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_ep_path = \"/mdf_open\"\n",
    "# source_ep_path = \"/mdf_open/kearns_biofilm_rupture_location_v1.1/Biofilm Images/Paper Images/Isofluence Images (79.4)/THY+75mM AA\"\n",
    "\n",
    "# source_ep_id = \"e38ee745-6d04-11e5-ba46-22000b92c6ec\"\n",
    "source_ep_id = \"82f1b5c6-6e9b-11e5-ba47-22000b92c6ec\"\n",
    "dest_ep_id = \"1adf6602-3e50-11ea-b965-0e16720bb42f\"  # where they will be extracted\n",
    "\n",
    "# Globus endpoint and file path at which we want to store metadata documents\n",
    "mdata_ep_id = \"5113667a-10b4-11ea-8a67-0e35e66293c2\"\n",
    "mdata_path = \"/projects/DLHub/mdf_metadata\"  # TODO: Add exception if you put slash at the end of the mdata_path. \n",
    "\n",
    "# FuncX endpoint at which we want the metadata extraction to occur. Does NOT have to be same endpoint as the data.\n",
    "#. funcx_ep_id = \"6045fcfb-c3ef-48db-9b32-5b50fda15144\"  # Path to funcX running on JetStream. \n",
    "funcx_ep_id = \"82ceed9f-dce1-4dd1-9c45-6768cf202be8\"  # River k8s cluster. \n",
    "\n",
    "# print(type(gdrive_pkl))\n",
    "\n",
    "# TODO: Need Google Credentials\n",
    "xtract = requests.post(f'http://127.0.0.1:5000/extract', data=pickle.dumps({\n",
    "                                                  'crawl_id': crawl_id,\n",
    "                                                  'gdrive_pkl': auth_creds,\n",
    "                                                  'headers': json.dumps(headers),\n",
    "                                                  'funcx_eid': funcx_ep_id, \n",
    "                                                  'source_eid': source_ep_id,\n",
    "                                                  'dest_eid': dest_ep_id,\n",
    "                                                  'mdata_store_path': mdata_path}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (plz)",
   "language": "python",
   "name": "plz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
