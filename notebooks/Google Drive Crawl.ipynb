{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling and Extracting Files from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# Standard Py Imports\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import os.path\n",
    "import requests\n",
    "\n",
    "# Google Imports\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Crawling\n",
    "...is how we create our initial index of files. Downstream, this lets us strategize as to how many and what types of files are processed at different locations. Here we use the **Google OAuth2 InstalledAppFlow** to get a user's Auth credentials, and then use the **Google Drive API** to generate a list of all files and their attributes from the Google Drive API. \n",
    "\n",
    "You may need to install the Google Drive requirements before running this notebook. In the terminal this can be done with: \n",
    "\n",
    "```pip install -r google_drive_nb_requirements.txt```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, set the following elements as environment variables on your machine\n",
    "# ** DISCLAIMER **: if you push these to GitHub, I will be sad.\n",
    "# ** FOR MACOSX: After you source, .bash_profile, might need full system restart to discover env vars. \n",
    "project_id = os.environ[\"goog_project_id\"]\n",
    "client_id = os.environ[\"goog_client_id\"]\n",
    "client_secret = os.environ[\"goog_client_secret\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawl_url = \"http://xtract-crawler-4.eba-ghixpmdf.us-east-1.elasticbeanstalk.com/crawl\"\n",
    "# xtract_url = \"http://127.0.0.1:5000\"\n",
    "xtract_url = \"http://xtractv1-env-2.p6rys5qcuj.us-east-1.elasticbeanstalk.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'web': {'auth_uri': 'https://accounts.google.com/o/oauth2/auth', 'token_uri': 'https://oauth2.googleapis.com/token', 'auth_provider_x509_cert_url': 'https://www.googleapis.com/oauth2/v1/certs', 'redirect_uris': ['http://localhost', 'http://localhost/potato', 'http://localhost:8080/Callback'], 'javascript_origins': ['http://localhost:8080']}}\n"
     ]
    }
   ],
   "source": [
    "# Add the secret stuff to our credentials document...\n",
    "with open(\"config.json\", \"r\") as f:\n",
    "    creds = json.load(f)\n",
    "print(creds)\n",
    "creds['web']['client_id'] = client_id\n",
    "creds['web']['client_secret'] = client_secret\n",
    "creds['web']['project_id'] = project_id\n",
    "\n",
    "# And write it.\n",
    "with open(\"credentials.json\", \"w\") as f: \n",
    "    json.dump(creds, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to define and run the login flow. Here we give our credentials to the Google OAuth2 API server, which will give back an immutable Credentials() object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCOPES = ['https://www.googleapis.com/auth/drive.metadata.readonly', 'https://www.googleapis.com/auth/drive.readonly']\n",
    "\n",
    "# Stolen from Google Quickstart docs\n",
    "# https://developers.google.com/drive/api/v3/quickstart/python\n",
    "def do_login_flow():\n",
    "    creds = None\n",
    "    # The file token.pickle stores the user's access and refresh tokens, and is\n",
    "    # created automatically when the authorization flow completes for the first\n",
    "    # time.\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'credentials.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "\n",
    "    return creds, None  # Returning None because Tyler can't figure out how he wants to structure this yet. \n",
    "\n",
    "# THIS should force-open a Google Auth window in your local browser. If not, you can manually copy-paste it. \n",
    "auth_creds = do_login_flow()\n",
    "\n",
    "# Now delete the file so you don't accidentally `git add` it. \n",
    "os.remove(\"credentials.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawl Started! Here's your trackable crawl_id:\n",
      " {'crawl_id': '83f5f2aa-0d6b-4ab2-9be2-8393eda87684'}\n"
     ]
    }
   ],
   "source": [
    "r = requests.post(url=f\"{xtract_url}/crawl\",\n",
    "                  data=pickle.dumps({'auth_creds': auth_creds, 'repo_type': 'GDRIVE'}) )\n",
    "\n",
    "crawl_mdata = json.loads(r.content)\n",
    "print(f\"Crawl Started! Here's your trackable crawl_id:\\n {crawl_mdata}\")\n",
    "crawl_id = crawl_mdata['crawl_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Extraction\n",
    "...is how we create our initial index of files. Downstream, this lets us strategize as to how many and what types of files are processed at different locations. Here we use the **Google OAuth2 InstalledAppFlow** to get a user's Auth credentials, and then use the **Google Drive API** to generate a list of all files and their attributes from the Google Drive API. \n",
    "\n",
    "You may need to install the Google Drive requirements before running this notebook. In the terminal this can be done with: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'crawl_end_t': 1600885692.551743, 'crawl_start_t': 1600885646.4216552, 'crawl_status': 'COMPLETED', 'gdrive_mdata': {'doc_types': {'is_gdoc': 1181, 'is_user_upload': 1944}, 'first_ext_tallies': {'compressed': 6, 'hierarch': 1, 'images': 376, 'other': 362, 'presentation': 183, 'tabular': 210, 'text': 1987}}, 'groups_crawled': 3125, 'n_commit_threads': 5, 'repo_type': 'GDrive', 'total_crawl_time': 46.13008785247803}\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(url=f'{xtract_url}/get_crawl_status', json={'crawl_id': crawl_id})\n",
    "print(json.loads(r.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please paste the following URL in a browser:\n",
      "https://auth.globus.org/v2/oauth2/authorize?client_id=7414f0b4-7d05-4bb6-bb00-076fa3f17cf5&redirect_uri=https%3A%2F%2Fauth.globus.org%2Fv2%2Fweb%2Fauth-code&scope=https%3A%2F%2Fauth.globus.org%2Fscopes%2F56ceac29-e98a-440a-a594-b41e7a084b62%2Fall+urn%3Aglobus%3Aauth%3Ascope%3Atransfer.api.globus.org%3Aall+https%3A%2F%2Fauth.globus.org%2Fscopes%2Ffacd7ccc-c5f4-42aa-916b-a0e270e2c2a9%2Fall+email+openid&state=_default&response_type=code&code_challenge=YsOl47N73FkNOl4iD0c_2fxMekHxawO5DsjTPRDIbTI&code_challenge_method=S256&access_type=online&prefill_named_grant=My+App+Login\n",
      "Please Paste your Auth Code Below: \n",
      "2dv3MOrT3iCCPrCkwz5WmaTGAyr8tE\n",
      "Headers: {'Authorization': 'Bearer AgXxne31dGqzG67v7K8Jo2qykld88rw9KYBwJzXxG0nl8Jr5wnHwCvjn50140wOxG8vE5b9b16va8aC49bdXoU8B5d', 'Transfer': 'Ag481yJ5qV03eWVW7gbry2JvWdojJ47EnwpedvqNO2zDXyb8v9C7Cqj0PEx4wz4PylQrajJGpOPxrMHj7PxB3t8r3X', 'FuncX': 'AgwjYDYD4VK8oPwrywjVvkxwY4bE5PDENQDezG3BBMp3ea2oDxflCg98r1W7G1YK92NY9jn0D9llMYc2M1rGEhy1Wa', 'Petrel': 'AgXxne31dGqzG67v7K8Jo2qykld88rw9KYBwJzXxG0nl8Jr5wnHwCvjn50140wOxG8vE5b9b16va8aC49bdXoU8B5d'}\n"
     ]
    }
   ],
   "source": [
    "# Do a Globus Login (Will won't need this, because of REFRESH token)\n",
    "from fair_research_login import NativeClient\n",
    "\n",
    "client = NativeClient(client_id='7414f0b4-7d05-4bb6-bb00-076fa3f17cf5')\n",
    "tokens = client.login(\n",
    "    requested_scopes=['https://auth.globus.org/scopes/56ceac29-e98a-440a-a594-b41e7a084b62/all', \n",
    "                      'urn:globus:auth:scope:transfer.api.globus.org:all',\n",
    "                     \"https://auth.globus.org/scopes/facd7ccc-c5f4-42aa-916b-a0e270e2c2a9/all\", \n",
    "                     'email', 'openid'],\n",
    "    no_local_server=True,\n",
    "    no_browser=True)\n",
    "\n",
    "auth_token = tokens[\"petrel_https_server\"]['access_token']\n",
    "transfer_token = tokens['transfer.api.globus.org']['access_token']\n",
    "funcx_token = tokens['funcx_service']['access_token']\n",
    "\n",
    "headers = {'Authorization': f\"Bearer {auth_token}\", 'Transfer': transfer_token, 'FuncX': funcx_token, 'Petrel': auth_token}\n",
    "print(f\"Headers: {headers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'crawl_end_t': 1600885692.551743, 'crawl_start_t': 1600885646.4216552, 'crawl_status': 'COMPLETED', 'gdrive_mdata': {'doc_types': {'is_gdoc': 1181, 'is_user_upload': 1944}, 'first_ext_tallies': {'compressed': 6, 'hierarch': 1, 'images': 376, 'other': 362, 'presentation': 183, 'tabular': 210, 'text': 1987}}, 'groups_crawled': 3125, 'n_commit_threads': 5, 'repo_type': 'GDrive', 'total_crawl_time': 46.13008785247803}\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(url=f'{xtract_url}/get_crawl_status', json={'crawl_id': crawl_id})\n",
    "print(json.loads(r.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting for crawl_id: 83f5f2aa-0d6b-4ab2-9be2-8393eda87684\n"
     ]
    }
   ],
   "source": [
    "# source_ep_path = \"/mdf_open\"\n",
    "# source_ep_path = \"/mdf_open/kearns_biofilm_rupture_location_v1.1/Biofilm Images/Paper Images/Isofluence Images (79.4)/THY+75mM AA\"\n",
    "\n",
    "# source_ep_id = \"e38ee745-6d04-11e5-ba46-22000b92c6ec\"\n",
    "source_ep_id = \"82f1b5c6-6e9b-11e5-ba47-22000b92c6ec\"\n",
    "dest_ep_id = \"1adf6602-3e50-11ea-b965-0e16720bb42f\"  # where they will be extracted\n",
    "\n",
    "# Globus endpoint and file path at which we want to store metadata documents\n",
    "mdata_ep_id = \"5113667a-10b4-11ea-8a67-0e35e66293c2\"\n",
    "mdata_path = \"/projects/DLHub/mdf_metadata\"  # TODO: Add exception if you put slash at the end of the mdata_path. \n",
    "\n",
    "# FuncX endpoint at which we want the metadata extraction to occur. Does NOT have to be same endpoint as the data.\n",
    "#. funcx_ep_id = \"6045fcfb-c3ef-48db-9b32-5b50fda15144\"  # Path to funcX running on JetStream. \n",
    "funcx_ep_id = \"68bade94-bf58-4a7a-bfeb-9c6a61fa5443\"  # River k8s cluster. \n",
    "\n",
    "# print(type(gdrive_pkl))\n",
    "\n",
    "# TODO: Need Google Credentials\n",
    "print(f\"Extracting for crawl_id: {crawl_id}\")\n",
    "xtract = requests.post(f'{xtract_url}/extract', data=pickle.dumps({\n",
    "                                                  'crawl_id': crawl_id,\n",
    "                                                  'gdrive_pkl': auth_creds,\n",
    "                                                  'headers': json.dumps(headers),\n",
    "                                                  'funcx_eid': funcx_ep_id, \n",
    "                                                  'source_eid': source_ep_id,\n",
    "                                                  'dest_eid': dest_ep_id,\n",
    "                                                  'mdata_store_path': mdata_path}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 100 metadata elements for crawl_id: 8ef87b04-bf89-45ec-848b-4d0b3ebfee8e\n",
      "b'{\"crawl_id\":\"8ef87b04-bf89-45ec-848b-4d0b3ebfee8e\",\"file_ls\":[],\"num_files\":0,\"queue_empty\":true}\\n'\n"
     ]
    }
   ],
   "source": [
    "# Start fetching your damn metadata\n",
    "n = 100\n",
    "print(f\"Fetching {n} metadata elements for crawl_id: {crawl_id}\")\n",
    "xtract = requests.get(f'{xtract_url}/fetch_mdata', json={'crawl_id': crawl_id, 'n': 1000})\n",
    "print(xtract.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (plz)",
   "language": "python",
   "name": "plz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
