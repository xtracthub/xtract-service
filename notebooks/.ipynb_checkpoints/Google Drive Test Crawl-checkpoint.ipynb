{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Drive Preliminary Tests\n",
    "\n",
    "This script is meant to crawl a Google Drive repository to find some base metrics. The goal of this notebook is to identify users whose Google Drive accounts are 'large' and 'interesting' for use in a full metadata extraction study. All Google Drive data crawled by this notebook is confidential and is not downloaded or stored to any computing machinery outside of Google. \n",
    "\n",
    "Note: all Drive accounts must come from an **xxx@uchicago.edu*** email address.\n",
    "\n",
    "To begin, you should run the following in the directory containing this notebook AND ensure your environment is using Python3.6+: \n",
    "\n",
    "`pip install -r nb-requirements.txt`\n",
    "\n",
    "After installing to your environment of choice, you may need to restart your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# Standard Py Imports\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "# Google Imports\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Auth\n",
    "We will Authenticate using Google OAuth2's InstalledAppFlow. You will likely need to replace the `project_id`, `client_id`, and `client_secret` with the fields provided by Tyler (you can just Copy+Paste them into this notebook so long as you don't push to GitHub). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the secret stuff to our credentials document...\n",
    "with open(\"../examples/config.json\", \"r\") as f:\n",
    "    creds = json.load(f)\n",
    "\n",
    "# project_id = os.environ[\"goog_project_id\"]\n",
    "# client_id = os.environ[\"goog_client_id\"]\n",
    "# client_secret = os.environ[\"goog_client_secret\"]\n",
    "\n",
    "project_id = \"TODO 1 of 3\"\n",
    "client_id = \"TODO 2 of 3\"\n",
    "client_secret = \"3 of 3\" \n",
    "\n",
    "creds['web']['client_id'] = client_id\n",
    "creds['web']['client_secret'] = client_secret\n",
    "creds['web']['project_id'] = project_id\n",
    "\n",
    "# And write out to file. \n",
    "with open(\"credentials.json\", \"w\") as f: \n",
    "    json.dump(creds, f)\n",
    "    \n",
    "SCOPES = ['https://www.googleapis.com/auth/drive.metadata.readonly', 'https://www.googleapis.com/auth/drive.readonly']\n",
    "\n",
    "# Stolen from Google Quickstart docs\n",
    "# https://developers.google.com/drive/api/v3/quickstart/python\n",
    "def do_login_flow():\n",
    "    creds = None\n",
    "    # The file token.pickle stores the user's access and refresh tokens, and is\n",
    "    # created automatically when the authorization flow completes for the first\n",
    "    # time.\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'credentials.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "\n",
    "    return creds, None  # Returning None because Tyler can't figure out how he wants to structure this yet. \n",
    "\n",
    "# This should force-open a Google Auth window in your local browser. \n",
    "#    If not, you can manually copy-paste it. \n",
    "auth_creds = do_login_flow()\n",
    "os.remove(\"credentials.json\")  # The worst way to provide the minimum level of file-levelsecurity.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Crawl \n",
    "The crawler is sending Drive API calls and is collecting information about every file and directory in your repository. In this, it is also trying to identify a 'best' first extractor to apply to each of your files (if any). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawl ID info:\n",
      " {'crawl_id': 'ec8ac1e5-cd70-4e43-844f-6f3eb7ddd906'}\n"
     ]
    }
   ],
   "source": [
    "crawl_url = \"http://xtract-crawler-4.eba-ghixpmdf.us-east-1.elasticbeanstalk.com/crawl\"\n",
    "status_url = \"http://xtract-crawler-4.eba-ghixpmdf.us-east-1.elasticbeanstalk.com/get_crawl_status\"\n",
    "# crawl_url = \"http://127.0.0.1:5000/crawl\"\n",
    "# status_url = \"http://127.0.0.1:5000/get_crawl_status\"\n",
    "\n",
    "r = requests.post(url=crawl_url,\n",
    "                  data=pickle.dumps({'auth_creds': auth_creds, 'repo_type': 'GDRIVE'}) )\n",
    "\n",
    "crawl_mdata = json.loads(r.content)\n",
    "print(f\"Crawl ID info:\\n {crawl_mdata}\")\n",
    "crawl_id = crawl_mdata['crawl_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Get status\n",
    "**Run the following cell periodically**. When 'complete', please send the resulting JSON object to Tyler on Slack. The entire crawl shouldn't take more than 5 minutes. (It takes my personal Google Drive containing 3,100 files ~20-40 seconds per run). Google's Drive API returns a maximum of 100 files per API call, hence why you may notice 'groups_crawled' incrementing by a multiple of 100 whenever you update. \n",
    "\n",
    "The crawl_status codes are: STARTING->PROCESSING->COMMITTING->COMPLETED/FAILED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Crawl Status: {'bytes_crawled': None, 'crawl_end_t': 1594935882.402955, 'crawl_id': 'ec8ac1e5-cd70-4e43-844f-6f3eb7ddd906', 'crawl_start_t': 1594935848.2388718, 'crawl_status': 'COMPLETED', 'files_crawled': 3136, 'gdrive_mdata': {'doc_types': {'is_gdoc': 1122, 'is_user_upload': 2014}, 'first_ext_tallies': {'images': 377, 'other': 305, 'presentation': 143, 'tabular': 200, 'text': 2111}}, 'groups_crawled': 3136, 'n_commit_threads': 5, 'repo_type': 'GDrive', 'total_crawl_time': 34.16408324241638}\n"
     ]
    }
   ],
   "source": [
    "crawl_status = requests.get(status_url, json={'crawl_id': crawl_id})\n",
    "print(crawl_status)\n",
    "crawl_content = json.loads(crawl_status.content)\n",
    "print(f\"Crawl Status: {crawl_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When the final status is \"COMPLETED\", please send me the resulting JSON document. Thanks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (plz)",
   "language": "python",
   "name": "plz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
